new_beta = beta - step_multiplier * grad
while(L(sample, beta) + step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.1
subsample_frac = 0.1
beta = rep(Inf,p)
new_beta = rep(0,p)
i = 0
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) + step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.1
subsample_frac = 0.1
beta = rep(1,p)
new_beta = rep(0,p)
i = 0
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) + step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.1
subsample_frac = 0.01
beta = rep(1,p)
new_beta = rep(0,p)
i = 0
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) + step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.01
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
i = 0
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) + step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.01
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
i = 0
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
# step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) + step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.0001
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
i = 0
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
# step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) + step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.0001
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
i = 0
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
# step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) - step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.0001
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
i = 0
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) - step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.0001
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
print(L(data, beta))
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(grad*normalize_vec(grad))
step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) - step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
print("d")
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 1
tolerance = 0.0001
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
print(L(data, beta))
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(normalize_vec(grad)^2)
step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) - step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
print("d")
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 0.0001
tolerance = 0.0001
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
print(L(data, beta))
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(normalize_vec(grad)^2)
step_multiplier = 1 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) - step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
print("d")
}
}
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 0.0001
tolerance = 0.0001
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
print(L(data, beta))
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(normalize_vec(grad)^2)
step_multiplier = 0.0001 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) - step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
print("d")
}
}
L(data, beta)
L(data, real_beta)
step_length = 0.1
step_shrinkage = 1/2
step_multiplier = 0.0001
tolerance = 0.0001
subsample_frac = 1
beta = rep(1,p)
new_beta = rep(0,p)
while (abs(L(data, beta) - L(data, new_beta)) > tolerance) {
beta = new_beta
print(L(data, beta))
sample = sample_frac(data, subsample_frac) # stochastic gradient descent
grad = grad_L(sample, beta) # vector-valued
slope = sum(normalize_vec(grad)^2)
step_multiplier = 0.0001 # reset backtracking step search
new_beta = beta - step_multiplier * grad
while(L(sample, beta) - step_multiplier*slope < L(sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
# print("d")
}
}
sample(seq_along(1:5), 5)
sample(seq_along(1:5), 1)
sample(seq_along(1:5), 1)
sample(seq_along(1:5), 3)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
n_sample
while(loss + step_multiplier*slope < L(x_sample, y_sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
# print("d")
}
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
while(loss - (0.5)*step_multiplier*slope < L(x_sample, y_sample, new_beta)) {
step_multiplier = step_multiplier * step_shrinkage
new_beta = beta - step_multiplier * grad
# print("d")
}
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
L(x, y, beta)
L(x, y, real_beta)
sample(seq_along(y), 10)
s = sample(seq_along(y), 10)
x[s,]
y[s]
?sample
?sample
sample
View(sample)
View(sample)
rm(sample)
sample
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
Inf - Inf
NaN > 0
Inf + Inf
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
beta
new_beta
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
a
a
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
L(x, y, beta)
L(x, y, real_beta)
source('~/.active-rstudio-document')
L(x, y, real_beta)
data = tibble(
x = runif(1000, 0, 10),
y = sin(x) + rnorm(1000, sd=0.1)
) %>%
ggplot(aes(x,y)) +
geom_smooth(method="lm", se=F)
tibble(
x = runif(1000, 0, 10),
y = sin(x) + rnorm(1000, sd=0.1)
) %>%
ggplot(aes(x,y)) +
geom_smooth(method="lm", se=F)
tibble(
x = runif(1000, 0, 10),
y = sin(x) + rnorm(1000, sd=0.1)
) %>%
ggplot(aes(x,y)) +
geom_point()
geom_smooth(method="lm", se=F)
tibble(
x = runif(1000, 0, 10),
y = sin(x) + rnorm(1000, sd=0.1)
) %>%
ggplot(aes(x,y)) +
geom_point() +
geom_smooth(method="lm", se=F)
tibble(
x = runif(100, 0, 10),
y = sin(x) + rnorm(100, sd=0.1)
) %>%
ggplot(aes(x,y)) +
geom_point() +
geom_smooth(method="lm", se=F)
tibble(
x = runif(100, 0, 10),
y = sin(x) + rnorm(100, sd=0.1)
) %>%
ggplot(aes(x,y)) +
geom_point() +
geom_smooth(method="lm", se=F, color="purple")
n = 100
f = function(x1, x2) = x1 + 4*x2 - x1*x2 # conditional mean
mvrnorm(n, mu, Sigma) %>%
as_tibble() %>%
set_names(c("x1", "x2")) %>%
mutate(y = f(x1, x2) + rnorm(n))
?metrics
library(yardstick)
?metrics
?metrics_set
pp = tibble(
p = runif(100),
y = rbinom(100,0.5,1)
)
pp = tibble(
p = runif(100),
y = rbinom(100,1,0.5)
)
pp
library(yardstick)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), ~ifelse(., "TRUE", "FALSE")) %>%
set_metrics(sens, spec)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), ~ifelse(., "TRUE", "FALSE")) %>%
metric_set(sens, spec)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), ~ifelse(., "TRUE", "FALSE")) %>%
(metric_set(sens, spec))
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), ~ifelse(., "TRUE", "FALSE")) %>%
metrics(y, yhat)
metrics = metric_set(sens, spec)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), ~ifelse(., "TRUE", "FALSE")) %>%
metrics(y, yhat)
?sens
?spec
spec
metrics = metric_set(sens, spec)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), ~ifelse(., "TRUE", "FALSE")) %>%
metrics(y, yhat)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), ~ifelse(., "TRUE", "FALSE")) %>%
metrics(truth=y, estimate=yhat)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y),
~ifelse(., "TRUE", "FALSE") %>%
factor()) %>%
metrics(truth=y, estimate=yhat)
?factor
as.factor(c(0,0,1))
as.factor(c(1,0,0))
as.factor(c(T,F,F))
as.factor(c(F,T,T))
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), as.factor) %>%
metrics(truth=y, estimate=yhat)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), as.factor) %>%
glimpse()
pp = tibble(
p = runif(100),
y = rbinom(100,1,0.5)==1
)
metrics = metric_set(sens, spec)
pp %>%
mutate(yhat = p>0.5) %>%
mutate_at(vars(yhat, y), as.factor) %>%
metrics(truth=y, estimate=yhat)
as.factor(c(F,T,T)) %>% levels
